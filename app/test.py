from io import BytesIO
import cv2
import os
import numpy as np
import torch
import faiss
import json
from pathlib import Path
from google.cloud import storage
from transformers import CLIPProcessor, CLIPModel
from dotenv import load_dotenv
import google.generativeai as genai
from transformers import pipeline
from huggingface_hub import login
import timm
from PIL import Image
from torchvision import transforms
from typing import Optional
import logging
import textwrap
import re
import requests
import xml.etree.ElementTree as ET
from sklearn.metrics.pairwise import cosine_similarity
from collections import Counter

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

load_dotenv()
login(token=os.getenv("HUGGINGFACE_TOKEN"))

GCS_BUCKET = "storage3000image"
GCS_IMAGE_PATH = "uploaded_images/"
GCS_KEY_PATH = storage.Client.from_service_account_json("app/gsc-key.json")

VECTOR_FILE = "static/processed/embedded_vectors.json"
GCS_FOLDER = "handle_data"
GCS_DATASET = f"dataset"
GCS_DATASET_PATH = f"{GCS_DATASET}/dataset.json"
GCS_INDEX_PATH = f"{GCS_FOLDER}/faiss_index.bin"
GCS_LABELS_PATH = f"{GCS_FOLDER}/labels.npy"
GCS_TEXT_INDEX_PATH = f"{GCS_FOLDER}/faiss_text_index.bin"
GCS_TEXT_LABELS_PATH = f"{GCS_FOLDER}/text_labels.npy"
GCS_ANOMALY_INDEX_PATH = f"{GCS_FOLDER}/faiss_index_anomaly.bin"
GCS_ANOMALY_LABELS_PATH = f"{GCS_FOLDER}/labels_anomaly.npy"
LOCAL_INDEX_PATH = "app/static/faiss/faiss_index.bin"
LOCAL_LABELS_PATH = "app/static/labels/labels.npy"
LOCAL_TEXT_INDEX_PATH = "app/static/faiss/faiss_text_index.bin"
LOCAL_TEXT_LABELS_PATH = "app/static/labels/text_labels.npy"
LOCAL_ANOMALY_INDEX_PATH = "app/static/faiss/faiss_index_anomaly.bin"
LOCAL_ANOMALY_LABELS_PATH = "app/static/labels/labels_anomaly.npy"
LOCAL_DATASET_PATH = "app/static/json/dataset.json"
INDEX_DIM = 512

index = None
labels = []
anomaly_index = None
anomaly_labels = []

device = "cuda" if torch.cuda.is_available() else "cpu"
model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32").to(device)
vit_model = timm.create_model("vit_base_patch16_224", pretrained=True).to(device)
vit_model.eval()
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
genai.configure(api_key=os.getenv("GEMINI_API_KEY"))

def download_from_gcs():
    storage_client = GCS_KEY_PATH
    bucket = storage_client.bucket(GCS_BUCKET)
    files_to_download = [
        (GCS_INDEX_PATH, LOCAL_INDEX_PATH),
        (GCS_LABELS_PATH, LOCAL_LABELS_PATH),
        (GCS_ANOMALY_INDEX_PATH, LOCAL_ANOMALY_INDEX_PATH),
        (GCS_ANOMALY_LABELS_PATH, LOCAL_ANOMALY_LABELS_PATH),
        (GCS_DATASET_PATH, LOCAL_DATASET_PATH),
    ]
    for gcs_path, local_path in files_to_download:
        blob = bucket.blob(gcs_path)
        os.makedirs(os.path.dirname(local_path), exist_ok=True)
        blob.download_to_filename(local_path)
        print(f"T·∫£i v·ªÅ {gcs_path} to {local_path}")

def preprocess_image(image_path: str) -> Optional[np.ndarray]:
    """Ti·ªÅn x·ª≠ l√Ω ·∫£nh b·∫±ng Gaussian Blur v√† Canny Edge Detection."""
    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
    if img is None:
        print(f"L·ªói: Kh√¥ng th·ªÉ ƒë·ªçc ·∫£nh t·ª´ {image_path}")
        return None
    blurred = cv2.GaussianBlur(img, (5, 5), 0)
    equalized = cv2.equalizeHist(blurred)
    edges = cv2.Canny(equalized, 50, 150)
    return edges

def embed_image(image_path: str) -> Optional[np.ndarray]:
    """Nh√∫ng ·∫£nh th√†nh vector s·ª≠ d·ª•ng m√¥ h√¨nh CLIP."""
    image = cv2.imread(image_path)
    if image is None:
        print(f"L·ªói: Kh√¥ng th·ªÉ ƒë·ªçc ·∫£nh t·ª´ {image_path}")
        return None
    inputs = processor(images=image, return_tensors="pt").to(device)
    with torch.no_grad():
        embedding = model.get_image_features(**inputs)
    return embedding.cpu().numpy().astype(np.float32)

def generate_anomaly_map(image_data: bytes) -> Optional[np.ndarray]:
    """Sinh anomaly map t·ª´ ·∫£nh ƒë·∫ßu v√†o b·∫±ng ViT feature extractor."""
    try:
        img = Image.open(BytesIO(image_data)).convert("RGB")
        original_size = img.size
        transform = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
        ])
        img_tensor = transform(img).unsqueeze(0).to(device)
        with torch.no_grad():
            features = vit_model.forward_features(img_tensor)
        feature_map = features.mean(dim=1).squeeze().cpu().numpy()
        anomaly_map = (feature_map - np.min(feature_map)) / (np.ptp(feature_map) + 1e-6)
        anomaly_map = (anomaly_map * 255).astype(np.uint8)
        anomaly_map_resized = cv2.resize(anomaly_map, original_size, interpolation=cv2.INTER_CUBIC)
        return anomaly_map_resized
    except Exception as e:
        print(f"L·ªói t·∫°o Anomaly Map: {e}")
        return None

def embed_anomaly_map(anomaly_map: np.ndarray) -> Optional[np.ndarray]:
    """Nh√∫ng anomaly map th√†nh vector s·ª≠ d·ª•ng m√¥ h√¨nh CLIP."""
    if anomaly_map is None:
        print("Anomaly map is None.")
        return None
    if len(anomaly_map.shape) == 2:  # Grayscale image
        anomaly_map_rgb = cv2.cvtColor(anomaly_map, cv2.COLOR_GRAY2RGB)
    else:
        anomaly_map_rgb = anomaly_map
    inputs = processor(images=anomaly_map_rgb, return_tensors="pt").to(device)
    with torch.no_grad():
        embedding = model.get_image_features(**inputs)
    return embedding.cpu().numpy().astype(np.float32)

def extract_disease_named(results: list) -> list:
    """Tr√≠ch xu·∫•t t√™n b·ªánh t·ª´ danh s√°ch k·∫øt qu·∫£."""
    simplified = []
    for item in results:
        try:
            if isinstance(item, str):
                label = item.split('/')[-1] if '/' in item else item
                simplified.append({
                    'label': label,
                    'cosine_similarity': 0.0
                })
            elif isinstance(item, dict) and 'label' in item:
                label = item['label'].split('/')[-1] if '/' in item['label'] else item['label']
                cosine_similarity = item.get('cosine_similarity', 0.0)
                simplified.append({
                    'label': label,
                    'cosine_similarity': float(cosine_similarity)
                })
            else:
                print(f"L·ªói khi x·ª≠ l√Ω item {item}: ƒê·ªãnh d·∫°ng kh√¥ng h·ª£p l·ªá")
        except Exception as e:
            print(f"L·ªói khi x·ª≠ l√Ω item {item}: {e}")
    return simplified

def load_faiss_index():
    """T·∫£i FAISS Index v√† nh√£n b·ªánh t·ª´ file."""
    global index, labels, anomaly_index, anomaly_labels
    if os.path.exists(LOCAL_INDEX_PATH):
        try:
            index = faiss.read_index(LOCAL_INDEX_PATH)
            print(f"FAISS Index t·∫£i th√†nh c√¥ng! T·ªïng s·ªë vector: {index.ntotal}")
        except Exception as e:
            print(f"L·ªói t·∫£i FAISS Index: {e}")
            index = None
    else:
        print("FAISS Index kh√¥ng t·ªìn t·∫°i!")

    if os.path.exists(LOCAL_ANOMALY_INDEX_PATH):
        try:
            anomaly_index = faiss.read_index(LOCAL_ANOMALY_INDEX_PATH)
            print(f"FAISS Anomaly Index t·∫£i th√†nh c√¥ng! T·ªïng s·ªë vector: {anomaly_index.ntotal}")
        except Exception as e:
            print(f"L·ªói t·∫£i FAISS Anomaly Index: {e}")
            anomaly_index = None
    else:
        print("FAISS Anomaly Index kh√¥ng t·ªìn t·∫°i!")
    
    if os.path.exists(LOCAL_ANOMALY_LABELS_PATH):
        anomaly_labels = np.load(LOCAL_ANOMALY_LABELS_PATH, allow_pickle=True).item()
        print(f"ƒê√£ t·∫£i {len(anomaly_labels)} nh√£n b·ªánh t·ª´ labels-anomaly.npy")
    else:
        print("labels-anomaly.npy kh√¥ng t·ªìn t·∫°i!")
    
    if os.path.exists(LOCAL_LABELS_PATH):
        labels = np.load(LOCAL_LABELS_PATH, allow_pickle=True).item()
        print(f"ƒê√£ t·∫£i {len(labels)} nh√£n b·ªánh t·ª´ labels.npy")
    else:
        print("labels.npy kh√¥ng t·ªìn t·∫°i!")

def search_similar_images(query_vector, top_k=5):
    """T√¨m ·∫£nh t∆∞∆°ng t·ª± b·∫±ng FAISS Index."""
    if index is None or index.ntotal == 0:
        print("FAISS index tr·ªëng!")
        return []
    if index.ntotal != len(labels):
        print(f"S·ªë vector ({index.ntotal}) kh√¥ng kh·ªõp v·ªõi nh√£n ({len(labels)}). K·∫øt qu·∫£ c√≥ th·ªÉ kh√¥ng ch√≠nh x√°c!")
    try:
        if query_vector.ndim == 1:
            query_vector = np.expand_dims(query_vector, axis=0)
        faiss.normalize_L2(query_vector)
        distances, indices = index.search(query_vector, top_k)
        print(f"Ch·ªâ s·ªë t√¨m th·∫•y: {indices}")
        print(f"Cosine similarities: {distances}")
        similar_results = []
        labels_keys = list(labels.keys()) if labels else []
        for idx, sim in zip(indices[0], distances[0]):
            print(f"X·ª≠ l√Ω idx: {idx}, similarity: {sim}")
            if sim < 55:  # Ng∆∞·ª°ng l·ªçc
                continue
            if 0 <= idx < len(labels_keys):
                label_filename = labels_keys[idx]
                label = labels.get(label_filename, "unknown")
                similar_results.append({
                    "label": label,
                    "cosine_similarity": float(sim)
                })
            else:
                print(f"Index {idx} v∆∞·ª£t ph·∫°m vi labels ({len(labels_keys)})!")
        return similar_results
    except Exception as e:
        print(f"L·ªói t√¨m ki·∫øm ·∫£nh t∆∞∆°ng t·ª±: {e}")
        return []

def search_anomaly_images(query_vector, top_k=5):
    """T√¨m ·∫£nh anomaly map t∆∞∆°ng t·ª± b·∫±ng FAISS Index."""
    if anomaly_index is None or anomaly_index.ntotal == 0:
        print("FAISS Anomaly Index tr·ªëng!")
        return []
    if anomaly_index.ntotal != len(anomaly_labels):
        print(f"S·ªë vector ({anomaly_index.ntotal}) kh√¥ng kh·ªõp v·ªõi nh√£n b·∫•t th∆∞·ªùng ({len(anomaly_labels)}). K·∫øt qu·∫£ c√≥ th·ªÉ kh√¥ng ch√≠nh x√°c!")
    try:
        if query_vector.ndim == 1:
            query_vector = np.expand_dims(query_vector, axis=0)
        faiss.normalize_L2(query_vector)
        distances, indices = anomaly_index.search(query_vector, top_k)
        print(f"Ch·ªâ s·ªë t√¨m th·∫•y (anomaly): {indices}")
        print(f"Cosine similarities (anomaly): {distances}")
        similar_results = []
        anomaly_labels_keys = list(anomaly_labels.keys()) if anomaly_labels else []
        for idx, sim in zip(indices[0], distances[0]):
            print(f"X·ª≠ l√Ω idx (anomaly): {idx}, similarity: {sim}")
            if sim < 55:  # Ng∆∞·ª°ng l·ªçc
                continue
            if 0 <= idx < len(anomaly_labels_keys):
                label_filename = anomaly_labels_keys[idx]
                label = anomaly_labels.get(label_filename, "unknown")
                similar_results.append({
                    "label": label,
                    "cosine_similarity": float(sim)
                })
            else:
                print(f"Index {idx} v∆∞·ª£t ph·∫°m vi labels_anomaly ({len(anomaly_labels_keys)})!")
        return similar_results
    except Exception as e:
        print(f"L·ªói t√¨m ki·∫øm ·∫£nh anomaly: {e}")
        return []

VALID_KEYWORDS = {
    "location": ["tay", "ch√¢n", "ƒë·∫ßu g·ªëi", "c·ªï tay", "b·ª•ng", "l∆∞ng", "m·∫∑t", "c·ªï", "ng·ª±c"],
    "duration": ["ng√†y", "tu·∫ßn", "th√°ng", "nƒÉm", "h√¥m nay", "h√¥m qua", "v√†i ng√†y", "l√¢u r·ªìi", 1, 2, 3, 4, 5, 6, 7, 8, 9],
    "appearance": ["ƒë·ªè", "s∆∞ng", "m∆∞ng m·ªß", "m·ª•n", "n·ªïi m·∫©n", "tr√≥c v·∫£y", "th√¢m"],
    "feeling": ["ng·ª©a", "ƒëau", "r√°t", "nh·ª©c", "kh√≥ ch·ªãu"],
    "spreading": ["c√≥", "kh√¥ng"],
}

def extract_keywords(text, field):
    if not isinstance(text, str):
        text = str(text)
    keywords = {
        "location": ["tay", "ch√¢n", "ƒë·∫ßu g·ªëi", "c·ªï tay", "ng√≥n", "m·∫∑t", "b·ª•ng", "l∆∞ng"],
        "duration": ["ng√†y", "tu·∫ßn", "th√°ng", "nƒÉm"],
        "appearance": ["ƒë·ªè", "s∆∞ng", "m·ª•n", "lo√©t", "n·ªïi c·ª•c", "v√†ng", "tr·∫Øng"],
        "feeling": ["ng·ª©a", "ƒëau", "r√°t", "kh√≥ ch·ªãu"],
        "spreading": ["c√≥", "kh√¥ng"]
    }
    matched = [word for word in keywords.get(field, []) if word in text.lower()]
    return ", ".join(matched) if matched else text

def collect_user_description():
    print("Thu th·∫≠p m√¥ t·∫£ c·ªßa b·ªánh nh√¢n (ho·∫∑c nh·∫•n Enter ƒë·ªÉ b·ªè qua):\n")
    try:
        print("B·∫Øt ƒë·∫ßu thu th·∫≠p m√¥ t·∫£ b·ªánh, b·ªánh nh√¢n vui l√≤ng tr·∫£ l·ªùi c√°c c√¢u h·ªèi sau:")
        location = input("Cho m√¨nh h·ªèi b·∫°n, b·∫°n c√≥ th·ªÉ cho bi·∫øt v·ªã tr√≠ c·ªßa b·ªánh kh√¥ng? (V√≠ d·ª•: ƒë·∫ßu g·ªëi, c·ªï tay,...)\n")
        duration = input("Th·ªùi gian b·∫°n b·ªã b·ªánh l√† bao l√¢u r·ªìi? (V√≠ d·ª•: 1 tu·∫ßn, 2 th√°ng,...)\n")
        appearance = input("H√¨nh d·∫°ng c·ªßa b·ªánh nh∆∞ th·∫ø n√†o? (V√≠ d·ª•: ƒë·ªè, s∆∞ng,...)\n")
        feeling = input("B·∫°n c·∫£m th·∫•y nh∆∞ th·∫ø n√†o? (V√≠ d·ª•: ƒëau, ng·ª©a,...)\n")
        spreading = input("B·ªánh c√≥ lan r·ªông kh√¥ng? (V√≠ d·ª•: c√≥, kh√¥ng)\n")
        location = extract_keywords(location, "location")
        duration = extract_keywords(duration, "duration")
        appearance = extract_keywords(appearance, "appearance")
        feeling = extract_keywords(feeling, "feeling")
        spreading = extract_keywords(spreading, "spreading")
        description = (
            f"Tri·ªáu ch·ª©ng xu·∫•t hi·ªán ·ªü {location}, ƒë√£ k√©o d√†i {duration}. "
            f"V√πng da c√≥ bi·ªÉu hi·ªán {appearance} v√† c·∫£m gi√°c {feeling}. "
            f"Tri·ªáu ch·ª©ng: {spreading} lan r·ªông.")
        print("ƒê√¢y l√† m√¥ t·∫£ b·ªánh b·∫°n ƒë√£ cung c·∫•p:\n")
        print(description)
        confirm = input("B·∫°n c√≥ mu·ªën x√°c nh·∫≠n m√¥ t·∫£ n√†y kh√¥ng? (y/n): ").strip().lower()
        if confirm == 'y':
            print("M√¥ t·∫£ b·ªánh c·ªßa b·∫°n ƒë√£ ƒë∆∞·ª£c ghi nh·∫≠n")
            return description
        else:
            retry = input("B·∫°n mu·ªën nh·∫≠p l·∫°i m√¥ t·∫£ b·ªánh? (y/n): ").strip().lower()
            if retry == 'y':
                return collect_user_description()
            else:
                print("M√¥ t·∫£ b·ªánh ƒë√£ b·ªã b·ªè qua.")
                return None
    except Exception as e:
        print(f"L·ªói thu th·∫≠p m√¥ t·∫£: {e}")
        return None

def generate_description_with_Gemini(image_path: str) -> Optional[str]:
    try:
        img = Image.open(image_path)
        model = genai.GenerativeModel('gemini-2.0-flash')
        prompt = """
        M√¥ t·∫£ b·ª©c ·∫£nh n√†y b·∫±ng ti·∫øng Vi·ªát, ƒë√¢y l√† ·∫£nh y khoa n√™n h√£y m√¥ t·∫£ th·∫≠t k·ªπ.
        Ch·ªâ t·∫≠p trung v√†o m√¥ t·∫£ l√¢m s√†ng, kh√¥ng ƒë∆∞a ra ch·∫©n ƒëo√°n hay k·∫øt lu·∫≠n.
        H√£y m√¥ t·∫£ c√°c ƒë·∫∑c ƒëi·ªÉm sau:
        - V·ªã tr√≠ c·ªßa t·ªïn th∆∞∆°ng (v√≠ d·ª•: l√≤ng b√†n tay, mu b√†n tay, ng√≥n ch√¢n...)
        - K√≠ch th∆∞·ªõc t·ªïn th∆∞∆°ng (∆∞·ªõc l∆∞·ª£ng theo mm ho·∫∑c cm)
        - M√†u s·∫Øc (ƒë·ªìng nh·∫•t hay nhi·ªÅu m√†u, ƒë·ªè, t√≠m, h·ªìng, v.v.)
        - K·∫øt c·∫•u b·ªÅ m·∫∑t da (m·ªãn, s·∫ßn s√πi, c√≥ v·∫£y, lo√©t...)
        - ƒê·ªô r√µ n√©t c·ªßa c√°c c·∫°nh t·ªïn th∆∞∆°ng (r√µ r√†ng hay m·ªù, lan t·ªèa)
        - T√≠nh ƒë·ªëi x·ª©ng (t·ªïn th∆∞∆°ng c√≥ ƒë·ªëi x·ª©ng 2 b√™n hay kh√¥ng)
        - Ph√¢n b·ªë (r·∫£i r√°c, t·∫≠p trung th√†nh ƒë√°m, theo ƒë∆∞·ªùng‚Ä¶)
        - C√°c ƒë·∫∑c ƒëi·ªÉm b·∫•t th∆∞·ªùng kh√°c n·∫øu c√≥ (ch·∫£y m√°u, v·∫£y, m·ª•n n∆∞·ªõc, s∆∞ng n·ªÅ‚Ä¶)
        Ch·ªâ m√¥ t·∫£ nh·ªØng g√¨ c√≥ th·ªÉ nh√¨n th·∫•y trong ·∫£nh, kh√¥ng ƒë∆∞a ra gi·∫£ ƒë·ªãnh hay ch·∫©n ƒëo√°n y khoa.
        X√≥a mark down v√† c√°c k√Ω t·ª± ƒë·∫∑c bi·ªát trong k·∫øt qu·∫£.
        """
        response = model.generate_content([prompt, img])
        caption = response.text.replace("\n", " ")
        return caption
    except Exception as e:
        print(f"L·ªói khi t·∫°o caption v·ªõi Gemini: {e}")
        return None

def combine_labels(anomaly_labels: list, normal_labels: list) -> list:
    """G·ªôp danh s√°ch nh√£n t·ª´ ·∫£nh g·ªëc v√† ·∫£nh anomaly."""
    combined = []
    for label in anomaly_labels + normal_labels:
        if isinstance(label, dict):
            combined.append(label['label'])
        else:
            combined.append(label)
    return combined

def generate_medical_entities(user_description: str, image_description: str) -> Optional[str]:
    combined_description = f"1. M√¥ t·∫£ t·ª´ ng∆∞·ªùi d√πng: {user_description}. 2. M√¥ t·∫£ t·ª´ ·∫£nh: {image_description}."
    print(combined_description)
    prompt = textwrap.dedent(f"""
        T√¥i c√≥ 2 ƒëo·∫°n m√¥ t·∫£ sau v·ªÅ m·ªôt v√πng da b·ªã b·∫•t th∆∞·ªùng: {combined_description}
        H√£y chu·∫©n h√≥a c·∫£ hai m√¥ t·∫£, lo·∫°i b·ªè t·ª´ d∆∞ th·ª´a, h·ª£p nh·∫•t l·∫°i, v√† tr√≠ch xu·∫•t c√°c ƒë·∫∑c tr∆∞ng y khoa quan tr·ªçng.
        M·ªói ƒë·∫∑c tr∆∞ng n√™n ƒë∆∞·ª£c g·∫Øn nh√£n thu·ªôc m·ªôt trong ba lo·∫°i sau:
        - "Tri·ªáu ch·ª©ng": m√¥ t·∫£ bi·ªÉu hi·ªán, d·∫•u hi·ªáu l√¢m s√†ng (v√≠ d·ª•: ph√°t ban, ng·ª©a, ƒë·ªè, bong tr√≥c‚Ä¶)
        - "V·ªã tr√≠ xu·∫•t hi·ªán": v√πng c∆° th·ªÉ b·ªã ·∫£nh h∆∞·ªüng (v√≠ d·ª•: mu b√†n tay, c·∫≥ng ch√¢n, ng√≥n tay‚Ä¶)
        - "Nguy√™n nh√¢n": y·∫øu t·ªë g√¢y ra t√¨nh tr·∫°ng ƒë√≥ n·∫øu c√≥ xu·∫•t hi·ªán trong m√¥ t·∫£ (v√≠ d·ª•: c√¥n tr√πng c·∫Øn, d·ªã ·ª©ng, ti·∫øp x√∫c h√≥a ch·∫•t‚Ä¶)
        Tr·∫£ v·ªÅ k·∫øt qu·∫£ d·∫°ng JSON Array. M·ªói ph·∫ßn t·ª≠ l√† m·ªôt object g·ªìm:
        - "entity": c·ª•m t·ª´ y khoa
        - "type": "Tri·ªáu ch·ª©ng", "V·ªã tr√≠ xu·∫•t hi·ªán", ho·∫∑c "Nguy√™n nh√¢n"
        V√≠ d·ª• ƒë·∫ßu ra:
        [
          {{ "entity": "v·∫øt ƒë·ªè", "type": "Tri·ªáu ch·ª©ng" }},
          {{ "entity": "c·∫≥ng ch√¢n", "type": "V·ªã tr√≠ xu·∫•t hi·ªán" }},
          {{ "entity": "d·ªã ·ª©ng th·ªùi ti·∫øt", "type": "Nguy√™n nh√¢n" }}
        ]
        Ch·ªâ li·ªát k√™ c√°c ƒë·∫∑c tr∆∞ng c√≥ trong m√¥ t·∫£. Kh√¥ng suy lu·∫≠n th√™m.
    """)
    try:
        model = genai.GenerativeModel('gemini-2.0-flash')
        response = model.generate_content([prompt])
        text = response.text.strip()
        clean_text = re.sub(r"```(?:json)?|```", "", text).strip()
        result = json.loads(clean_text)
        decoded_result = json.dumps(result, ensure_ascii=False)
        return decoded_result
    except Exception as e:
        print(f"L·ªói khi t·∫°o m√¥ t·∫£ v·ªõi Gemini: {e}")
        return None

def compare_descriptions_and_labels(description: str, labels: list) -> list:
    labels_str = ", ".join(labels)
    prompt = textwrap.dedent(f"""
        M√¥ t·∫£: "{description}"
        Nh√£n: "{labels_str}"
        So s√°nh s·ª± kh√°c bi·ªát gi·ªØa m√¥ t·∫£ v√† nh√£n b·ªánh. Sau ƒë√≥, t·∫°o ra 3 c√¢u h·ªèi gi√∫p ph√¢n bi·ªát ch√≠nh x√°c h∆°n.
        Tr·∫£ v·ªÅ k·∫øt qu·∫£ theo ƒë·ªãnh d·∫°ng:
        1. ...
        2. ...
        3. ...
    """)
    try:
        model = genai.GenerativeModel('gemini-2.0-flash')
        response = model.generate_content([prompt])
        text = response.text.strip()
        questions = re.findall(r"\d+\.\s+(.*)", text)
        return questions
    except Exception as e:
        print(f"L·ªói khi g·ªçi Gemini: {e}")
        return []

def ask_user_questions(questions: list, disease_name: str) -> str:
    answers = []
    for idx, q in enumerate(questions, 1):
        print(f"C√¢u {idx}: {q}")
        answer = answer_question(q, disease_name)
        answers.append(f"C√¢u h·ªèi: {q}\nTr·∫£ l·ªùi: {answer}")
    return "\n\n".join(answers)

def answer_question(question: str, disease_name: str) -> str:
    prompt = f"""
    H√£y t∆∞·ªüng t∆∞·ª£ng b·∫°n l√† m·ªôt b·ªánh nh√¢n ƒëang b·ªã b·ªánh {disease_name}.
    H√£y tr·∫£ l·ªùi c√¢u h·ªèi sau v·ªÅ tri·ªáu ch·ª©ng c·ªßa b·∫°n: {question} n√™n nh·ªõ l√† tr·∫£ l·ªùi theo c√°ch c·ªßa m·ªôt b·ªánh nh√¢n ƒëang m·∫Øc b·ªánh {disease_name}.
    Tr·∫£ l·ªùi "c√≥" ho·∫∑c "kh√¥ng" v√† gi·∫£i th√≠ch l√Ω do t·∫°i sao b·∫°n l·∫°i tr·∫£ l·ªùi nh∆∞ v·∫≠y h√£y gi·∫£i th√≠ch nh∆∞ 1 con ng∆∞·ªùi.
    N·∫øu c√¢u tr·∫£ l·ªùi m√† b·∫°n kh√¥ng ƒë∆∞a ra ƒë∆∞·ª£c th√¨ ch·ªçn ng·∫´u nhi√™n gi·ªØa c√≥ v√† kh√¥ng
    Lo·∫°i b·ªè markdown v√† c√°c k√Ω t·ª± kh√¥ng c·∫ßn thi·∫øt trong c√¢u tr·∫£ l·ªùi c·ªßa b·∫°n. v√≠ d·ª• "```json" , "```",**,...
    """
    try:
        model = genai.GenerativeModel("gemini-2.0-flash")
        response = model.generate_content(prompt)
        result = response.text.strip()
        result = re.sub(r"^(?:\w+)?\n|\n$", "", result).strip()
        result = re.sub(r"\n+", " ", result).strip()
        if not result:
            return "Kh√¥ng c√≥ th√¥ng tin"
        return result
    except Exception as e:
        print(f"L·ªói khi t·ªïng h·ª£p th√¥ng tin: {e}")
        return "X·∫£y ra l·ªói trong qu√° tr√¨nh t·ªïng h·ª£p th√¥ng tin"

def clean_image_name(image_name: str) -> str:
    name = os.path.splitext(image_name)[0]
    name = re.sub(r"\(\d+\)", "", name)
    return name.strip().lower()

def process_image(image_data: bytes) -> tuple:
    if not image_data:
        print("Kh√¥ng c√≥ d·ªØ li·ªáu ·∫£nh ƒë·ªÉ x·ª≠ l√Ω.")
        return None, [], [], [], []
    
    print("ƒê√£ nh·∫≠n d·ªØ li·ªáu ·∫£nh, b·∫Øt ƒë·∫ßu x·ª≠ l√Ω...")
    
    # Convert bytes to image for embedding
    nparr = np.frombuffer(image_data, np.uint8)
    img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
    if img is None:
        print("L·ªói gi·∫£i m√£ ·∫£nh, d·ª´ng quy tr√¨nh.")
        return None, [], [], [], []

    # Save image temporarily for embed_image
    processed_dir = Path("app/static/processed")
    processed_dir.mkdir(parents=True, exist_ok=True)
    temp_image_path = processed_dir / "temp_image.jpg"
    cv2.imwrite(str(temp_image_path), img)

    embedding = embed_image(str(temp_image_path))
    result_labels_simple = []
    detailed_labels_normal = []
    if embedding is not None:
        detailed_labels_normal = search_similar_images(embedding)
        print("detailed_labels_normal", detailed_labels_normal)
        detailed_labels_normal = extract_disease_named(detailed_labels_normal)
        print("detailed_labels_normal sau khi c·∫Øt", detailed_labels_normal)
        result_labels_simple = [item["label"] for item in detailed_labels_normal]
        print("result_labels_simple", result_labels_simple)
        print("üîç ·∫¢nh g·ªëc:")
        for item in detailed_labels_normal:
            print(f"- {item['label']} (cosine: {item['cosine_similarity']:.4f})")

    anomaly_map = generate_anomaly_map(image_data)
    print("Anomaly map generated:", anomaly_map is not None)
    anomaly_result_labels_simple = []
    detailed_labels_anomaly = []
    if anomaly_map is not None:
        anomaly_map_path = processed_dir / "anomaly_map_temp.jpg"
        cv2.imwrite(str(anomaly_map_path), anomaly_map)
        anomaly_map_embedding = embed_anomaly_map(anomaly_map)
        if anomaly_map_embedding is not None:
            detailed_labels_anomaly = search_anomaly_images(anomaly_map_embedding)
            print("detailed_labels_anomaly", detailed_labels_anomaly)
            detailed_labels_anomaly = extract_disease_named(detailed_labels_anomaly)
            print("detailed_labels_anomaly sau khi c·∫Øt", detailed_labels_anomaly)
            anomaly_result_labels_simple = [item["label"] for item in detailed_labels_anomaly]
            print("üîç Anomaly Map:")
            for item in detailed_labels_anomaly:
                print(f"- {item['label']} (cosine: {item['cosine_similarity']:.4f})")

    final_labels = combine_labels(detailed_labels_anomaly, detailed_labels_normal)
    print("final_labels", final_labels)

    # Cleanup temporary files
    for temp_file in [temp_image_path, anomaly_map_path]:
        try:
            if temp_file and temp_file.exists():
                temp_file.unlink()
        except Exception as e:
            print(f"L·ªói khi x√≥a file t·∫°m {temp_file}: {e}")
    
    return final_labels, result_labels_simple, anomaly_result_labels_simple, detailed_labels_normal, detailed_labels_anomaly

def filter_incorrect_labels_by_user_description(description: str, labels: list) -> dict:
    labels_str = ", ".join(labels)
    prompt = textwrap.dedent(f"""
        M√¥ t·∫£ b·ªánh c·ªßa ng∆∞·ªùi d√πng: "{description}"
        Danh s√°ch c√°c nh√£n b·ªánh nghi ng·ªù: [{labels_str}]

        Nhi·ªám v·ª•:
        1. Ph√¢n t√≠ch m√¥ t·∫£ v√† so s√°nh v·ªõi t·ª´ng nh√£n b·ªánh.
        2. Lo·∫°i b·ªè c√°c nh√£n b·ªánh kh√¥ng ph√π h·ª£p v·ªõi m√¥ t·∫£. Gi·∫£i th√≠ch l√Ω do lo·∫°i b·ªè r√µ r√†ng.
        3. Gi·ªØ l·∫°i c√°c nh√£n ph√π h·ª£p nh·∫•t, s·∫Øp x·∫øp theo m·ª©c ƒë·ªô ph√π h·ª£p gi·∫£m d·∫ßn.

        K·∫øt qu·∫£ ƒë·∫ßu ra ph·∫£i ·ªü ƒë·ªãnh d·∫°ng JSON:
        {{
            "loai_bo": [{{"label": "nh√£n kh√¥ng ph√π h·ª£p", "ly_do": "..."}}],
            "giu_lai": [{{"label": "nh√£n ph√π h·ª£p", "do_phu_hop": "cao/trung b√¨nh/th·∫•p"}}]
        }}
    """)
    try:
        model = genai.GenerativeModel('gemini-2.0-flash')
        response = model.generate_content([prompt])
        text = response.text.strip()
        clean_text = re.sub(r"```(?:json)?|```", "", text).strip()
        result = json.loads(clean_text)
        return result
    except Exception as e:
        print(f"L·ªói khi t·∫°o m√¥ t·∫£ v·ªõi Gemini: {e}")
        return {"loai_bo": [], "giu_lai": []}

def upload_json_to_gcs(bucket_name: str, destination_blob_name: str, source_file_name: str):
    client = storage.Client.from_service_account_json("app/gsc-key.json")
    bucket = client.bucket(bucket_name)
    blob = bucket.blob(destination_blob_name)
    blob.upload_from_filename(source_file_name)
    print(f"Uploaded {source_file_name} to {destination_blob_name}.")

def append_disease_to_json(file_path: str, new_disease: dict):
    with open(file_path, 'r+', encoding='utf-8') as f:
        try:
            data = json.load(f)
        except json.JSONDecodeError:
            data = []
        data.append(new_disease)
        f.seek(0)
        json.dump(data, f, ensure_ascii=False, indent=2)
        f.truncate()
    print(f"Added new disease: {new_disease.get('name', '')}")

def search_disease_in_json(file_path: str, disease_name: str) -> list:
    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    if not isinstance(data, list):
        print("File JSON kh√¥ng ph·∫£i l√† danh s√°ch.")
        return []
    results = [
        entry for entry in data
        if isinstance(entry, dict) and disease_name.lower() in entry.get("T√™n b·ªánh", "").lower()
    ]
    return results

def generate_keyword(keyword: str) -> str:
    prompt = f"""
    B·∫°n l√† m·ªôt ng∆∞·ªùi c√≥ ki·∫øn th·ª©c s√¢u r·ªông v·ªÅ y khoa d·ª±a v√†o keyword ƒë∆∞·ª£c truy·ªÅn v√†o.
    Keyword l√† t√™n b·ªánh t√¥i c·∫ßn b·∫°n t·∫°o ra danh s√°ch t√™n b·ªánh li√™n quan ƒë·∫øn keyword ƒë√≥.
    V√≠ d·ª• t√™n b·ªánh l√†: Squamouscell th√¨ b·∫°n c√≥ th·ªÉ li·ªát k√™ c√°c keyword li√™n quan ƒë·∫øn t√™n nh∆∞: Squamouse, Squamouse cell, Squamouse Cancer,..
    T·ªëi ƒëa l√† 10 t·ª´ kh√≥a li√™n quan ƒë·∫øn t√™n b·ªánh ƒë√≥.
    Tr·∫£ v·ªÅ d∆∞·ªõi d·∫°ng json v·ªõi c·∫•u tr√∫c nh∆∞ sau:
    {{
        "keyword": [
            "keyword1",
            "keyword2",
            "keyword3",
            "keyword4",
            "keyword5",
            "keyword6",
            "keyword7",
            "keyword8",
            "keyword9",
            "keyword10"
        ]
    }}
    """
    try:
        if not keyword:
            return "Kh√¥ng c√≥ t·ª´ kh√≥a truy·ªÅn v√†o"
        model = genai.GenerativeModel("gemini-2.0-flash")
        response = model.generate_content(prompt)
        result = response.text.strip()
        result = re.sub(r"^(?:\w+)?\n|\n$", "", result).strip()
        if not result:
            return "Kh√¥ng c√≥ th√¥ng tin"
        return result
    except Exception as e:
        print(f"L·ªói khi t·∫°o t·ª´ kh√≥a: {e}")
        return "X·∫£y ra l·ªói trong qu√° tr√¨nh t·∫°o t·ª´ kh√≥a"

def search_medlineplus(ten_khoa_hoc: str) -> str:
    if not ten_khoa_hoc or ten_khoa_hoc == "Kh√¥ng t√¨m th·∫•y":
        return "Kh√¥ng c√≥ t√™n b·ªánh truy·ªÅn v√†o"
    print(f"T√¨m ki·∫øm th√¥ng tin b·ªánh '{ten_khoa_hoc}' tr√™n MedLinePlus...")
    keyword = generate_keyword(ten_khoa_hoc)
    try:
        keyword_dict = json.loads(keyword)
    except json.JSONDecodeError:
        keyword_dict = {"keyword": [ten_khoa_hoc]}
    
    url = 'https://wsearch.nlm.nih.gov/ws/query'
    params = {
        'db': 'healthTopics',
        'term': f'{ten_khoa_hoc}'
    }
    response = requests.get(url, params=params)
    if response.status_code == 200:
        print("T√¨m ki·∫øm th√†nh c√¥ng!")
        cleaned_content = clean_xml_content(response.content)
        return cleaned_content
    
    for item in keyword_dict.get("keyword", []):
        print(f"T√¨m ki·∫øm th√¥ng tin b·ªánh '{item}' tr√™n MedLinePlus...")
        params = {
            'db': 'healthTopics',
            'term': f'{item}'
        }
        response = requests.get(url, params=params)
        if response.status_code == 200:
            print("T√¨m ki·∫øm th√†nh c√¥ng!")
            cleaned_content = clean_xml_content(response.content)
            return cleaned_content
    return ""

def clean_xml_content(xml_content: str) -> str:
    """Nh·∫≠n v√†o m·ªôt chu·ªói XML, tr√≠ch xu·∫•t v√† k·∫øt h·ª£p n·ªôi dung vƒÉn b·∫£n t·ª´ t·∫•t c·∫£ c√°c ph·∫ßn t·ª≠."""
    try:
        root = ET.fromstring(xml_content)
        text_nodes = [elem.text.strip() for elem in root.iter() if elem.text and elem.text.strip()]
        return ' '.join(text_nodes)
    except ET.ParseError as e:
        print(f"L·ªói ph√¢n t√≠ch XML: {e}")
        return ""

def decide_final_label(label_string: list) -> str:
    counter = Counter(label_string)
    most_common = counter.most_common(1)
    return most_common[0][0] if most_common else "unknown"

def extract_medical_info(text: str) -> dict:
    prompt = f"""
    D·ªãch vƒÉn b·∫£n v·ªÅ ti·∫øng vi·ªát
    B·∫°n l√† m·ªôt chuy√™n gia y t·∫ø, b·∫°n c√≥ kh·∫£ nƒÉng tr√≠ch xu·∫•t th√¥ng tin y khoa t·ª´ vƒÉn b·∫£n.
    H√£y tr√≠ch xu·∫•t th√¥ng tin y khoa t·ª´ vƒÉn b·∫£n d∆∞·ªõi d·∫°ng JSON h·ª£p l·ªá **kh√¥ng ch·ª©a Markdown**.
    Ch·ªâ l·∫•y k·∫øt qu·∫£ ƒë·∫ßu ti√™n m√† b·∫°n t√¨m ƒë∆∞·ª£c
    H√£y tr√≠ch xu·∫•t th·∫≠t chi ti·∫øt
    VƒÉn b·∫£n ƒë·∫ßu v√†o l√†:
    {text}
    {{
        "T√™n b·ªánh":"", T√™n b·ªánh l√† t√™n c·ªßa b·ªánh ƒë∆∞·ª£c ghi ·ªü ƒë·∫ßu ti√™u ƒë·ªÅ
        "T√™n khoa h·ªçc": "", T√™n khoa h·ªçc th∆∞·ªùng ƒë∆∞·ª£c ghi b·∫±ng ti·∫øng anh ho·∫∑c ti·∫øng latinh, n·∫øu kh√¥ng th·∫•y th√¨ t√™n khoa h·ªçc s·∫Ω b·∫±ng t√™n b·ªánh nh∆∞ng ch·ªâ l·∫•y ph·∫ßn ti·∫øng anh
        "Tri·ªáu ch·ª©ng": "", Tri·ªáu ch·ª©ng l√† nh·ªØng d·∫•u hi·ªáu m√† b·ªánh nh√¢n c√≥ th·ªÉ g·∫∑p ph·∫£i khi m·∫Øc b·ªánh n√†y, n·∫øu kh√¥ng t√¨m th·∫•y th√¨ c√≥ th·ªÉ ƒë·ªïi t·ª´ t√™n b·ªánh sang ti·∫øng anh
        "V·ªã tr√≠ xu·∫•t hi·ªán": "", V·ªã tr√≠ xu·∫•t hi·ªán l√† n∆°i m√† b·ªánh n√†y c√≥ th·ªÉ x·∫£y ra tr√™n c∆° th·ªÉ ng∆∞·ªùi,
        "Nguy√™n nh√¢n": "",  Nguy√™n nh√¢n l√† l√Ω do m√† b·ªánh n√†y x·∫£y ra
        "Ti√™u ch√≠ ch·∫©n ƒëo√°n": "",  Ti√™u ch√≠ ch·∫©n ƒëo√°n l√† nh·ªØng ti√™u ch√≠ m√† b√°c sƒ© c√≥ th·ªÉ s·ª≠ d·ª•ng ƒë·ªÉ x√°c ƒë·ªãnh b·ªánh n√†y
        "Ch·∫©n ƒëo√°n ph√¢n bi·ªát": "",  Ch·∫©n ƒëo√°n ph√¢n bi·ªát l√† nh·ªØng b·ªánh kh√°c m√† b√°c sƒ© c√≥ th·ªÉ xem x√©t khi x√°c ƒë·ªãnh b·ªánh n√†y
        "ƒêi·ªÅu tr·ªã": "",  ƒêi·ªÅu tr·ªã l√† nh·ªØng ph∆∞∆°ng ph√°p m√† b√°c sƒ© c√≥ th·ªÉ s·ª≠ d·ª•ng ƒë·ªÉ ƒëi·ªÅu tr·ªã b·ªánh n√†y
        "Ph√≤ng b·ªánh": "" , Ph√≤ng b·ªánh l√† nh·ªØng bi·ªán ph√°p m√† b√°c sƒ© c√≥ th·ªÉ khuy√™n b·ªánh nh√¢n th·ª±c hi·ªán ƒë·ªÉ ngƒÉn ng·ª´a b·ªánh n√†y
        "C√°c lo·∫°i thu·ªëc":
        [{{
            "T√™n thu·ªëc": "", 
            "Li·ªÅu l∆∞·ª£ng": "", 
            "Th·ªùi gian s·ª≠ d·ª•ng": ""
        }}]
    }}
    - N·∫øu kh√¥ng c√≥ th√¥ng tin, ƒë·∫∑t gi√° tr·ªã "Kh√¥ng t√¨m th·∫•y".
    - Kh√¥ng th√™m gi·∫£i th√≠ch, kh√¥ng in Markdown, kh√¥ng th√™m k√Ω t·ª± th·ª´a.
    """
    try:
        model = genai.GenerativeModel("gemini-2.0-flash")
        response = model.generate_content(prompt)
        raw_text = response.text if hasattr(response, "text") else response.parts[0].text
        raw_text = re.sub(r"^```json\n|\n```$", "", raw_text)
        extracted_info = json.loads(raw_text)
        completed_info = clean_text_json(extracted_info)
        return completed_info
    except json.JSONDecodeError:
        print("L·ªói: Kh√¥ng th·ªÉ parse JSON t·ª´ Gemini.")
        return {}
    except Exception as e:
        print(f"L·ªói tr√≠ch xu·∫•t th√¥ng tin y khoa: {e}")
        return {}

def clean_text(text: str) -> str:
    """L√†m s·∫°ch m·ªôt chu·ªói vƒÉn b·∫£n."""
    if not isinstance(text, str):
        return text
    text = re.sub(r'[\\\n\r\t\*\]]', ' ', text)
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

def clean_text_json(data):
    """L√†m s·∫°ch to√†n b·ªô vƒÉn b·∫£n trong m·ªôt c·∫•u tr√∫c JSON."""
    if isinstance(data, dict):
        return {key: clean_text_json(value) for key, value in data.items()}
    elif isinstance(data, list):
        return [clean_text_json(item) for item in data]
    else:
        return clean_text(data)

def translate_disease_name(disease_name: str) -> str:
    prompt = f"""
    B·∫°n l√† m·ªôt chuy√™n gia y t·∫ø c√≥ hi·ªÉu bi·∫øt s√¢u r·ªông v·ªÅ y khoa.
    B·∫°n c√≥ kh·∫£ nƒÉng d·ªãch t√™n b·ªánh t·ª´ ti·∫øng anh sang ti·∫øng vi·ªát.
    T√™n b·ªánh ƒë∆∞·ª£c truy·ªÅn v√†o l√†: {disease_name}
    H√£y d·ªãch t√™n b·ªánh ƒë√≥ sang ti·∫øng vi·ªát.
    Tr·∫£ v·ªÅ t√™n b·ªánh ƒë√≥
    """
    try:
        if not disease_name:
            return "Kh√¥ng c√≥ t√™n b·ªánh truy·ªÅn v√†o"
        model = genai.GenerativeModel("gemini-2.0-flash")
        response = model.generate_content(prompt)
        result = response.text.strip()
        result = re.sub(r"^(?:\w+)?\n|\n$", "", result).strip()
        if not result:
            return "Ch√∫ng t√¥i kh√¥ng th·ªÉ d·ªãch t√™n b·ªánh n√†y"
        return result
    except Exception as e:
        print(f"L·ªói khi d·ªãch t√™n b·ªánh: {e}")
        return "X·∫£y ra l·ªói trong qu√° tr√¨nh d·ªãch t√™n b·ªánh"

def search_final(name: str):
    translate_name = translate_disease_name(name)
    print(f"T√™n b·ªánh ƒë√£ d·ªãch: {translate_name}")
    search_json_result = search_disease_in_json(LOCAL_DATASET_PATH, translate_name)
    if search_json_result:
        print(f"K·∫øt qu·∫£ t√¨m ki·∫øm trong file JSON: {search_json_result}")
    else:
        print(f"Kh√¥ng t√¨m th·∫•y t√™n b·ªánh '{translate_name}' trong file JSON.")
        print("B·∫Øt ƒë·∫ßu t√¨m ki·∫øm b·∫±ng MedlinePlus...")
        search_medline_result = search_medlineplus(name)
        print(f"K·∫øt qu·∫£ t√¨m ki·∫øm MedlinePlus: {search_medline_result}")
        print("B·∫Øt ƒë·∫ßu tr√≠ch xu·∫•t th√¥ng tin y khoa t·ª´ MedlinePlus...")
        extract_medical_info_result = extract_medical_info(search_medline_result)
        if extract_medical_info_result:
            print(f"K·∫øt qu·∫£ tr√≠ch xu·∫•t th√¥ng tin y khoa: {extract_medical_info_result}")
            print("ƒêang th√™m th√¥ng tin v√†o file JSON...")
            append_disease_to_json(LOCAL_DATASET_PATH, extract_medical_info_result)
            print("Upload file JSON l√™n GCS...")
            upload_json_to_gcs(GCS_BUCKET, GCS_DATASET_PATH, LOCAL_DATASET_PATH)

def generate_description(disease_name: str) -> Optional[str]:
    """Sinh m√¥ t·∫£ tri·ªáu ch·ª©ng c·ªßa b·ªánh nh√¢n d·ª±a tr√™n t√™n b·ªánh v·ªõi Gemini."""
    if not disease_name or not isinstance(disease_name, str):
        print("T√™n b·ªánh kh√¥ng h·ª£p l·ªá.")
        return None
    prompt = f"""
    B·∫°n l√† m·ªôt b·ªánh nh√¢n ƒëang b·ªã b·ªánh {disease_name}.
    H√£y m√¥ t·∫£ tri·ªáu ch·ª©ng c·ªßa b·∫°n theo c√°c y·∫øu t·ªë sau:
    - V·ªã tr√≠ xu·∫•t hi·ªán tr√™n c∆° th·ªÉ
    - Th·ªùi gian k√©o d√†i
    - H√¨nh d·∫°ng v√† m√†u s·∫Øc c·ªßa v√πng b·ªã ·∫£nh h∆∞·ªüng
    - C·∫£m gi√°c m√† b·∫°n c·∫£m nh·∫≠n ƒë∆∞·ª£c (nh∆∞ ng·ª©a, r√°t, ƒëau...)
    - Tri·ªáu ch·ª©ng c√≥ lan r·ªông kh√¥ng
    H√£y tr·∫£ l·ªùi theo ƒë·ªãnh d·∫°ng sau:
    "Tri·ªáu ch·ª©ng xu·∫•t hi·ªán ·ªü [v·ªã tr√≠], ƒë√£ k√©o d√†i [th·ªùi gian]. 
    V√πng da c√≥ bi·ªÉu hi·ªán [h√¨nh d·∫°ng v√† m√†u s·∫Øc] v√† c·∫£m gi√°c [c·∫£m gi√°c]. 
    Tri·ªáu ch·ª©ng: [lan r·ªông hay kh√¥ng]."
    V√≠ d·ª•:
    "Tri·ªáu ch·ª©ng xu·∫•t hi·ªán ·ªü tay, ƒë√£ k√©o d√†i 2 tu·∫ßn. 
    V√πng da c√≥ bi·ªÉu hi·ªán ƒë·ªè v√† c·∫£m gi√°c ng·ª©a. Tri·ªáu ch·ª©ng: kh√¥ng lan r·ªông."
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash')
        response = model.generate_content([prompt])
        text = response.text.strip()
        return text
    except Exception as e:
        print(f"L·ªói khi t·∫°o m√¥ t·∫£ v·ªõi Gemini: {e}")
        return None

def process_pipeline(image_path: str, disease_name: str) -> tuple:
    """X·ª≠ l√Ω ·∫£nh v√† sinh k·∫øt qu·∫£ ch·∫©n ƒëo√°n."""
    try:
        with open(image_path, "rb") as f:
            image_data = f.read()
    except FileNotFoundError:
        print(f"L·ªói: File ·∫£nh {image_path} kh√¥ng t·ªìn t·∫°i.")
        return None, None, None, None
    
    final_labels, result_labels, anomaly_result_labels, detailed_labels_normal, detailed_labels_anomaly = process_image(image_data)
    if final_labels is None:
        print(f"L·ªói x·ª≠ l√Ω ·∫£nh {image_path}, b·ªè qua...")
        return None, None, None, None
    
    user_description = generate_description(disease_name)
    print("M√¥ t·∫£ t·ª´ ng∆∞·ªùi d√πng:", user_description)
    if not user_description:
        print("\nƒêang ch·ªçn nh√£n d·ª±a tr√™n h√¨nh ·∫£nh v√† m√¥ h√¨nh...")
        final_diagnosis = decide_final_label(final_labels)
        print(f"\nK·∫øt qu·∫£ s∆° b·ªô: {final_diagnosis}")
        print("Th√¥ng b√°o: V√¨ b·∫°n kh√¥ng cung c·∫•p m√¥ t·∫£, k·∫øt qu·∫£ c√≥ th·ªÉ ch∆∞a ch√≠nh x√°c.")
        return final_diagnosis, user_description, [], []
    
    image_description = generate_description_with_Gemini(image_path)
    print("M√¥ t·∫£ t·ª´ ·∫£nh (Gemini):", image_description)
    result_medical_entities = generate_medical_entities(user_description, image_description)
    print("Medical entities:", result_medical_entities)
    
    questions = compare_descriptions_and_labels(result_medical_entities, final_labels)
    print("Generated questions:", questions)
    if not questions:
        print("Kh√¥ng t·∫°o ƒë∆∞·ª£c c√¢u h·ªèi ph√¢n bi·ªát.")
        return final_labels, user_description, [], []
    
    print("\n--- Tr·∫£ l·ªùi c√°c c√¢u h·ªèi ƒë·ªÉ ph√¢n bi·ªát b·ªánh ---")
    user_answers = ask_user_questions(questions, disease_name)
    
    print("\n--- M√¥ t·∫£ b·ªï sung t·ª´ ng∆∞·ªùi d√πng ---")
    print(user_answers)
    combined_description = f"{result_medical_entities}\n\n{user_answers}"
    print("\n--- ƒêang lo·∫°i tr·ª´ nh√£n kh√¥ng ph√π h·ª£p ---")
    result = filter_incorrect_labels_by_user_description(combined_description, final_labels)
    if not result:
        print("Kh√¥ng c√≥ k·∫øt qu·∫£ t·ª´ Gemini.")
        return final_labels, user_description, questions, user_answers
    
    refined_labels = result.get("giu_lai", [])
    if not refined_labels:
        print("Kh√¥ng c√≤n nh√£n n√†o ph√π h·ª£p. ƒê·ªÅ xu·∫•t tham kh·∫£o b√°c sƒ©.")
    else:
        print("C√°c nh√£n c√≤n l·∫°i sau lo·∫°i tr·ª´:")
        for label_info in refined_labels:
            label = label_info.get("label")
            ket_qua = "-".join(label.split("-")[1:]) if "-" in label else label
            suitability = label_info.get("do_phu_hop")
            print(f"- {ket_qua} (M·ª©c ƒë·ªô ph√π h·ª£p: {suitability})")
            search_final(ket_qua)
    
    return refined_labels, user_description, questions, user_answers

def get_all_images(directory: str) -> list:
    exts = [".jpg", ".jpeg", ".png", ".bmp"]
    return [p for p in Path(directory).rglob("*") if p.is_file() and p.suffix.lower() in exts]

def test_process_pipeline(choice: str) -> list:
    image_dir = "app/static/data_test"
    file_path = "app/static/test_result.json"

    # L·∫•y danh s√°ch t·∫•t c·∫£ ·∫£nh v√† s·∫Øp x·∫øp ƒë·ªÉ ƒë·∫£m b·∫£o th·ª© t·ª± nh·∫•t qu√°n
    all_images = sorted(get_all_images(image_dir))
    if not all_images:
        print("[!] Kh√¥ng t√¨m th·∫•y ·∫£nh trong th∆∞ m·ª•c.")
        return []

    # Hi·ªÉn th·ªã danh s√°ch ·∫£nh ƒë·ªÉ ng∆∞·ªùi d√πng ch·ªçn
    print("\nDanh s√°ch ·∫£nh c√≥ s·∫µn:")
    for idx, img_path in enumerate(all_images, 1):
        print(f"{idx}. {os.path.basename(img_path)}")

    # Y√™u c·∫ßu ng∆∞·ªùi d√πng ch·ªçn ·∫£nh b·∫±ng s·ªë th·ª© t·ª±
    while True:
        try:
            if choice.lower() == 'q':
                print("ƒê√£ tho√°t ch∆∞∆°ng tr√¨nh.")
                return []
            choice = int(choice)
            if 1 <= choice <= len(all_images):
                break
            else:
                print(f"Vui l√≤ng nh·∫≠p s·ªë t·ª´ 1 ƒë·∫øn {len(all_images)}.")
        except ValueError:
            print("Vui l√≤ng nh·∫≠p m·ªôt s·ªë h·ª£p l·ªá ho·∫∑c 'q' ƒë·ªÉ tho√°t.")

    # L·∫•y ·∫£nh ƒë∆∞·ª£c ch·ªçn
    image_path = all_images[choice - 1]
    image_name = os.path.basename(image_path)
    image_name_cleaned = clean_image_name(image_name)

    # X·ª≠ l√Ω ·∫£nh ƒë∆∞·ª£c ch·ªçn
    print(f"\n=== ƒêang x·ª≠ l√Ω ·∫£nh: {image_path} ===")
    result, user_description, questions, user_answers = process_pipeline(str(image_path), image_name_cleaned)

    print(f"D·ª± ƒëo√°n: {result}")
    print(f"Th·ª±c t·∫ø: {image_name_cleaned}")

    # Ki·ªÉm tra ƒë√∫ng/sai
    is_correct = False
    if result:
        if isinstance(result, str):
            is_correct = image_name_cleaned.lower() == result.lower()
        elif isinstance(result, list):
            is_correct = any(image_name_cleaned.lower() == label_info.get("label", "").lower() for label_info in result)
    ket_qua = "ƒê√∫ng" if is_correct else "Sai"

    print(f"K·∫øt qu·∫£: {ket_qua}")
    print(f"ƒê∆∞·ªùng d·∫´n ·∫£nh: {image_path}")

    # ƒê·ªçc file JSON hi·ªán c√≥ (n·∫øu t·ªìn t·∫°i) ƒë·ªÉ l·∫•y danh s√°ch k·∫øt qu·∫£ c≈©
    existing_results = []
    if os.path.exists(file_path):
        try:
            with open(file_path, "r", encoding="utf-8") as file:
                existing_results = json.load(file)
                if not isinstance(existing_results, list):
                    print("File JSON kh√¥ng ƒë√∫ng ƒë·ªãnh d·∫°ng, kh·ªüi t·∫°o danh s√°ch m·ªõi.")
                    existing_results = []
        except json.JSONDecodeError:
            print("L·ªói ƒë·ªçc file JSON, kh·ªüi t·∫°o danh s√°ch m·ªõi.")
            existing_results = []

    # T·∫°o k·∫øt qu·∫£ m·ªõi
    new_result = {
        "STT": len(existing_results) + 1,  # TƒÉng STT d·ª±a tr√™n s·ªë l∆∞·ª£ng k·∫øt qu·∫£ hi·ªán c√≥
        "·∫¢nh": image_name,
        "D·ª± ƒëo√°n": result,
        "Th·ª±c t·∫ø": image_name_cleaned,
        "K·∫øt qu·∫£": ket_qua,
        "ƒê∆∞·ªùng d·∫´n": str(image_path),
        "M√¥ t·∫£ ng∆∞·ªùi d√πng": user_description,
        "C√¢u h·ªèi ph√¢n bi·ªát": questions,
        "Tr·∫£ l·ªùi c√¢u h·ªèi": user_answers
    }

    # Th√™m k·∫øt qu·∫£ m·ªõi v√†o danh s√°ch
    existing_results.append(new_result)

    # Ghi l·∫°i to√†n b·ªô danh s√°ch v√†o file JSON
    try:
        with open(file_path, "w", encoding="utf-8") as file:
            json.dump(existing_results, file, ensure_ascii=False, indent=4)
        print(f"\nHo√†n t·∫•t. K·∫øt qu·∫£ m·ªõi ƒë√£ ƒë∆∞·ª£c n·ªëi th√™m v√†o {file_path}.")
    except Exception as e:
        print(f"L·ªói khi ghi v√†o file {file_path}: {e}")

    return existing_results

def mainclient():
    download_from_gcs()
    load_faiss_index()
    for i in range(91,92):
        print(f"\n=== B·∫Øt ƒë·∫ßu ki·ªÉm tra ·∫£nh th·ª© {i} ===")
        results = test_process_pipeline(str(i))
        if not results:
            print("Kh√¥ng c√≥ k·∫øt qu·∫£ n√†o ƒë∆∞·ª£c t·∫°o ra.")
        else:
            print(f"ƒê√£ x·ª≠ l√Ω {len(results)} ·∫£nh, k·∫øt qu·∫£ ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o file JSON.")

if __name__ == "__main__":
    mainclient()